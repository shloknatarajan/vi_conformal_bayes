{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acc9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes,load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43967ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad613d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from cp_implementation\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import snakeviz\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "import random\n",
    "from sklearn.datasets import load_diabetes\n",
    "assert pyro.__version__.startswith('1.8.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3372118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sparse Regression ##\n",
    "#Well specified?\n",
    "# train_frac either 1 or 0.7 in their testing\n",
    "# dataset = \"diabetes\"\n",
    "# seed = 100 plus some number that changes with iterations\n",
    "train_frac = 0.7\n",
    "dataset = 'diabetes'\n",
    "seed = 100\n",
    "def load_traintest_sparsereg(train_frac, dataset,seed):\n",
    "    #Load dataset\n",
    "    if dataset ==\"diabetes\":\n",
    "        x,y = load_diabetes(return_X_y = True)\n",
    "    elif dataset ==\"boston\":\n",
    "        x,y = load_boston(return_X_y = True)\n",
    "    else:\n",
    "        print('Invalid dataset')\n",
    "        return\n",
    "\n",
    "    n = np.shape(x)[0]\n",
    "    d = np.shape(x)[1]\n",
    "\n",
    "    #Standardize beforehand (for validity)\n",
    "    x = (x - np.mean(x,axis = 0))/np.std(x,axis = 0)\n",
    "    y = (y - np.mean(y))/np.std(y)\n",
    "\n",
    "    #Train test split\n",
    "    ind_train, ind_test = train_test_split(np.arange(n), train_size = int(train_frac*n),random_state = seed)\n",
    "    x_train = x[ind_train]\n",
    "    y_train = y[ind_train]\n",
    "    x_test = x[ind_test]\n",
    "    y_test = y[ind_test]\n",
    "    \n",
    "    ### what is y_plot / what is it used for / where do -2 and +2100 come from?\n",
    "    y_plot = np.linspace(np.min(y_train) - 2, np.max(y_train) + 2,100)\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,y_plot,n,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "026a5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,x_test,y_test,y_plot,n,d = load_traintest_sparsereg(train_frac,dataset,seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a2d2c",
   "metadata": {},
   "source": [
    "y_plot are all the value of y_hat you will use in your loop.\n",
    "Goal: use their train test split and run your stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0757841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n",
      "309\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac494a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Guide Setup\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "    mu0 = torch.zeros(dim, dtype=torch.float64)\n",
    "    # sample f from the Beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.MultivariateNormal(mu0, std0))\n",
    "    # loop over the observed data\n",
    "    subset = random.sample(data, int(len(data) / dim))\n",
    "    for i in range(len(subset)):\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Normal(f.dot(data[i][0]), 0.3), obs=data[i][1])\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    mu_q = pyro.param(\"mu_q\", copy.deepcopy(prev_mu_q))\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.MultivariateNormal(mu_q, std0))\n",
    "\n",
    "def train_svi(D_hat, n_steps):\n",
    "    # setup the optimizer\n",
    "    adam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    # do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(D_hat)\n",
    "        if loss < 1e-5:\n",
    "            break\n",
    "    \n",
    "    mu_q = pyro.param(\"mu_q\")\n",
    "    return mu_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global prev_mu_q\n",
    "prev_mu_q = torch.zeros(dim, dtype=torch.float64)\n",
    "std0 = torch.eye(dim, dtype=torch.float64) * 0.3\n",
    "\n",
    "def run_svi(X_train, Y_train, y_plot, threshold=0.9):\n",
    "    # Run SVI on the Data\n",
    "    y_hat = max(Y_train)\n",
    "    y_bottom = min(Y_train)\n",
    "    print(y_hat)\n",
    "    print(y_bottom)\n",
    "    conformal_set = []\n",
    "    start = time.time()\n",
    "    for y_hat in y_plot: # changed from while loop\n",
    "        pyro.clear_param_store()\n",
    "        # Create D_hat\n",
    "        D_hat = list(zip(X_train[:-1], Y_train))\n",
    "        D_hat.append((X_train[-1], y_hat))\n",
    "        \n",
    "        # Train SVI\n",
    "        mu_q = train_svi(D_hat, 4)\n",
    "        prev_mu_q = mu_q\n",
    "        \n",
    "        # Calculate rank of y_hat\n",
    "        rank = [(abs(sum(D_hat[i][0] * mu_q) - D_hat[i][1]).detach().numpy()) for i in range(len(D_hat))]\n",
    "        y_hat_rank = rank[-1]\n",
    "        \n",
    "        # Add to conformal set if in not in bottom 10 percent of probabilities\n",
    "        if np.count_nonzero(y_hat_rank > rank) / len(rank) < threshold:\n",
    "            conformal_set.append(copy.deepcopy(y_hat))\n",
    "            print(f\"{y_hat} Added\")\n",
    "        else:\n",
    "            print(f\"{y_hat} Not added\")\n",
    "            \n",
    "        y_hat -= decrease_size\n",
    "    conformal_set = [min(conformal_set), max(conformal_set)]\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Conformal Set: [{float(conformal_set[0])}, {float(conformal_set[1])}]\")\n",
    "    print(f\"Length: {float(conformal_set[1] - conformal_set[0])}\")\n",
    "    print(f\"Y[-1]: {Y[-1]}\")\n",
    "    if Y[-1] >= conformal_set[0] and Y[-1] <= conformal_set[1]:\n",
    "        print(\"Y[-1] is covered\")\n",
    "    else:\n",
    "        print(\"Y[-1] is Not covered\")\n",
    "    print(f\"Elapsed Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a344385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loglikelihood(y,x):\n",
    "            return sp.stats._distn_infrastructure.pdf(y,loc = np.dot(beta_post[j],x.transpose())+ intercept_post[j],scale = sigma_post[j]) #compute likelihood samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0460fe3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats._distn_infrastructure' has no attribute 'pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1s/1mkc4t8137lbs9x8hhg9q8940000gp/T/ipykernel_67015/94508315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormal_loglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/1s/1mkc4t8137lbs9x8hhg9q8940000gp/T/ipykernel_67015/625245200.py\u001b[0m in \u001b[0;36mnormal_loglikelihood\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormal_loglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_post\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mintercept_post\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma_post\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#compute likelihood samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.stats._distn_infrastructure' has no attribute 'pdf'"
     ]
    }
   ],
   "source": [
    "normal_loglikelihood(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa840c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
